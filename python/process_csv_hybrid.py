"""
CSV Processor using Hybrid Neural Network Model for PanicSense
This script processes CSV files using a hybrid model approach.
It handles multilingual input (English and Tagalog) and outputs detailed sentiment analysis.
"""

import os
import sys
import json
import argparse
import time
from typing import Dict, List, Any, Optional, Tuple
import pandas as pd
import numpy as np
from datetime import datetime
import re
import csv
import json
import random  # For simulating model predictions in fallback mode
import langdetect  # For language detection

# Initialize sentiment categories
SENTIMENT_CATEGORIES = ['Panic', 'Fear/Anxiety', 'Resilience', 'Neutral', 'Disbelief']

# Setup disaster type keywords for classification
DISASTER_KEYWORDS = {
    'Earthquake': ['earthquake', 'quake', 'seismic', 'tremor', 'lindol', 'pagyanig'],
    'Flood': ['flood', 'flooding', 'baha', 'tubig', 'water rising', 'deluge'],
    'Typhoon': ['typhoon', 'hurricane', 'storm', 'cyclone', 'bagyo', 'unos', 'habagat'],
    'Fire': ['fire', 'burning', 'smoke', 'flames', 'sunog', 'apoy', 'usok'],
    'Volcanic Eruption': ['volcano', 'eruption', 'lava', 'ash', 'bulkan', 'lahar', 'taal', 'mayon'],
    'Landslide': ['landslide', 'mudslide', 'rockfall', 'collapse', 'pagguho', 'guho']
}

# Setup Philippine location keywords for location extraction
PH_LOCATIONS = [
    # Major cities and regions
    'Manila', 'Quezon City', 'Davao', 'Cebu', 'Makati', 'Taguig', 'Pasig', 'Pasay', 
    'Caloocan', 'ParaÃ±aque', 'Mandaluyong', 'Muntinlupa', 'Marikina', 'NCR', 'Metro Manila',
    'Luzon', 'Visayas', 'Mindanao',
    
    # Provinces
    'Batangas', 'Cavite', 'Laguna', 'Rizal', 'Bulacan', 'Pampanga', 'Zambales', 'Pangasinan',
    'Ilocos', 'Cagayan', 'Isabela', 'Nueva Ecija', 'Tarlac', 'Aurora', 'Bataan', 'Benguet',
    'Albay', 'Catanduanes', 'Camarines', 'Sorsogon', 'Marinduque', 'Quezon', 'Palawan',
    'Iloilo', 'Negros', 'Cebu', 'Bohol', 'Leyte', 'Samar',
    'Zamboanga', 'Misamis', 'Davao', 'Cotabato', 'Maguindanao', 'Lanao', 'Sulu', 'Basilan',
    
    # Key disaster-prone areas
    'Mayon', 'Taal', 'Pinatubo', 'Sierra Madre', 'Bicol', 'Cagayan Valley', 'CARAGA'
]

# Sentiment keywords for pattern matching fallback
SENTIMENT_KEYWORDS = {
    'Panic': [
        'emergency', 'help', 'run', 'evacuate', 'panic', 'scream', 'escape', 'flee', 'terror',
        'emergency', 'tulong', 'takbo', 'ligtas', 'sigaw', 'takot na takot', 'panik', 'tulungan'
    ],
    'Fear/Anxiety': [
        'scared', 'fear', 'worried', 'anxiety', 'nervous', 'afraid', 'concerned', 'frightened',
        'takot', 'natatakot', 'nag-aalala', 'kabado', 'nangangamba', 'balisa', 'pag-aalala'
    ],
    'Resilience': [
        'strong', 'survive', 'resilient', 'hope', 'endure', 'rebuild', 'recover', 'brave',
        'malakas', 'matatag', 'kakayanin', 'pag-asa', 'muling bumangon', 'magpakatatag', 'matapang'
    ],
    'Neutral': [
        'update', 'report', 'news', 'information', 'status', 'situation', 'alert', 'announced',
        'balita', 'ulat', 'impormasyon', 'kalagayan', 'sitwasyon', 'babala', 'anunsyo'
    ],
    'Disbelief': [
        'false', 'hoax', 'fake', 'rumor', 'exaggerated', 'unbelievable', 'doubt', 'really?',
        'hindi totoo', 'peke', 'kasinungalingan', 'tsismis', 'exaggerated', 'hindi kapani-paniwala'
    ]
}

class CSVProcessorHybrid:
    """
    Process CSV files using the hybrid neural network model
    Specifically designed for sentiment analysis in disaster contexts
    """
    
    def __init__(self, model_path=None):
        """
        Initialize the processor
        
        Args:
            model_path: Path to the pretrained model (if None, the default model will be used)
        """
        self.model_path = model_path
        self.location_extractor = LocationExtractor()
        self.disaster_type_extractor = DisasterTypeExtractor()
        
        # Script execution start time (for tracking)
        self.start_time = time.time()
        self.current_session_id = None
        
        print(f"Initialized CSV Processor with Hybrid Neural Network model")
        
        # Check if full model components are available
        try:
            import torch
            self.torch_available = True
            print("PyTorch is available, can use neural components")
        except ImportError:
            self.torch_available = False
            print("PyTorch is not available, will use fallback methods")
        
        try:
            import transformers
            self.transformers_available = True
            print("Transformers library is available, can use mBERT")
        except ImportError:
            self.transformers_available = False
            print("Transformers not available, will use pattern matching and ML fallbacks")
            
        # Initialize fallback pattern-based classifier
        self.initialize_fallback_classifier()
            
    def initialize_fallback_classifier(self):
        """Initialize the pattern-based fallback classifier"""
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.naive_bayes import MultinomialNB
        from sklearn.pipeline import Pipeline
        
        # Initialize a simple ML pipeline for sentiment analysis
        self.vectorizer = TfidfVectorizer(
            max_features=10000,
            ngram_range=(1, 2),
            stop_words='english'
        )
        
        # Use MultinomialNB for sentiment classification
        self.classifier = MultinomialNB()
        
        # Simple dataset for training the fallback classifier
        train_texts = []
        train_labels = []
        
        # Add examples for each sentiment category from keywords
        for sentiment, keywords in SENTIMENT_KEYWORDS.items():
            # Create synthetic examples from keywords
            for keyword in keywords:
                # Create multiple variations with the keyword
                train_texts.append(f"We are experiencing {keyword} during the disaster")
                train_texts.append(f"The situation is causing {keyword} among residents")
                train_texts.append(f"People are feeling {keyword}")
                train_texts.append(f"The disaster has led to {keyword}")
                train_labels.extend([sentiment] * 4)
        
        # Fit the vectorizer and classifier
        X = self.vectorizer.fit_transform(train_texts)
        self.classifier.fit(X, train_labels)
        
        print(f"Fallback classifier trained with {len(train_texts)} examples")
        
    def report_progress(self, processed: int, stage: str, total: int = None):
        """Print progress in a format that can be parsed by the Node.js service"""
        progress_data = {
            "processed": processed,
            "total": total,
            "stage": stage
        }
        # Format for easy parsing in Node.js
        print(f"PROGRESS:{json.dumps(progress_data)}::END_PROGRESS")
        sys.stdout.flush()
    
    def process_csv(self, input_file: str, output_file: Optional[str] = None, 
                   text_column: str = 'text', batch_size: int = 32, 
                   validate: bool = False) -> Dict[str, Any]:
        """
        Process a CSV file containing text data
        
        Args:
            input_file: Path to input CSV file
            output_file: Path to output JSON file (if None, uses input filename with .json extension)
            text_column: Name of column containing text data
            batch_size: Batch size for processing
            validate: Whether to validate model performance (requires 'sentiment' column)
            
        Returns:
            Dictionary with processing results and metrics
        """
        start_time = time.time()
        
        # Set default output file if not provided
        if not output_file:
            base_name = os.path.splitext(input_file)[0]
            output_file = f"{base_name}_analyzed.json"
        
        # Read CSV file
        try:
            print(f"Reading CSV file: {input_file}")
            df = pd.read_csv(input_file)
            total_rows = len(df)
            print(f"Found {total_rows} rows in CSV file")
            
            self.report_progress(0, f"Loaded CSV with {total_rows} records", total_rows)
        except Exception as e:
            error_msg = f"Error reading CSV file: {str(e)}"
            print(error_msg)
            return {"error": error_msg}
        
        # Check if text column exists
        if text_column not in df.columns:
            available_columns = ', '.join(df.columns)
            error_msg = f"Text column '{text_column}' not found in CSV. Available columns: {available_columns}"
            print(error_msg)
            return {"error": error_msg}
        
        # Initialize results storage
        results = []
        processed_count = 0
        
        # Metrics for evaluation
        true_labels = []
        predicted_labels = []
        confidence_scores = []
        has_ground_truth = validate and 'sentiment' in df.columns
        
        # Process in batches
        for i in range(0, total_rows, batch_size):
            batch = df.iloc[i:min(i + batch_size, total_rows)]
            
            # Extract text data from batch
            texts = batch[text_column].tolist()
            
            # Update progress
            batch_start = time.time()
            self.report_progress(
                processed_count, 
                f"Processing batch {i//batch_size + 1}/{(total_rows + batch_size - 1)//batch_size}", 
                total_rows
            )
            
            # Process each text in the batch
            batch_results = []
            for j, text in enumerate(texts):
                # Skip empty text
                if pd.isna(text) or text.strip() == '':
                    batch_results.append({
                        "text": "",
                        "sentiment": "Neutral",
                        "confidence": 0.0,
                        "language": "unknown",
                        "location": "",
                        "disaster_type": "Not Specified",
                        "explanation": "Empty text"
                    })
                    continue
                
                # Process the text
                text = str(text).strip()
                try:
                    # Detect language
                    try:
                        language = langdetect.detect(text)
                    except:
                        language = "unknown"
                        
                    # Get timestamp if available
                    timestamp = batch["timestamp"].iloc[j] if "timestamp" in batch.columns else None
                    if timestamp and not isinstance(timestamp, str):
                        timestamp = timestamp.strftime("%Y-%m-%d %H:%M:%S") if hasattr(timestamp, 'strftime') else str(timestamp)
                    
                    # Get source if available
                    source = batch["source"].iloc[j] if "source" in batch.columns else "CSV Import"
                    
                    # Analyze sentiment
                    result = self.analyze_sentiment(text, language)
                    
                    # Extract location and disaster type
                    location = self.location_extractor.extract(text)
                    disaster_type = self.disaster_type_extractor.extract(text)
                    
                    # Create result object
                    result_obj = {
                        "text": text,
                        "sentiment": result["sentiment"],
                        "confidence": result["confidence"],
                        "language": language,
                        "location": location,
                        "disaster_type": disaster_type,
                        "explanation": result["explanation"],
                        "timestamp": timestamp,
                        "source": source
                    }
                    
                    batch_results.append(result_obj)
                    
                    # Collect metrics if validation is enabled
                    if has_ground_truth:
                        true_sentiment = batch["sentiment"].iloc[j]
                        predicted_sentiment = result["sentiment"]
                        true_labels.append(true_sentiment)
                        predicted_labels.append(predicted_sentiment)
                        confidence_scores.append(result["confidence"])
                    
                except Exception as e:
                    print(f"Error processing text: {str(e)}")
                    batch_results.append({
                        "text": text,
                        "sentiment": "Neutral",
                        "confidence": 0.0,
                        "language": "unknown",
                        "location": "",
                        "disaster_type": "Not Specified",
                        "explanation": f"Error during processing: {str(e)}"
                    })
            
            # Add batch results to overall results
            results.extend(batch_results)
            processed_count += len(batch)
            
            # Calculate batch processing time
            batch_time = time.time() - batch_start
            records_per_second = len(batch) / batch_time if batch_time > 0 else 0
            
            # Report progress with timing information
            self.report_progress(
                processed_count,
                f"Processed {processed_count}/{total_rows} records - "
                f"{records_per_second:.2f} records/sec",
                total_rows
            )
        
        # Calculate metrics if validation is enabled
        metrics = None
        if has_ground_truth and true_labels:
            metrics = self.calculate_metrics(true_labels, predicted_labels, confidence_scores)
            print(f"Validation metrics: {json.dumps(metrics, indent=2)}")
        
        # Save results to JSON file
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "results": results,
                "metrics": metrics,
                "total_processed": processed_count,
                "processing_time": time.time() - start_time
            }, f, indent=2, ensure_ascii=False)
        
        print(f"Results saved to {output_file}")
        
        # Final progress update
        self.report_progress(
            processed_count,
            f"Analysis complete. Processed {processed_count} records in {time.time() - start_time:.2f} seconds",
            total_rows
        )
        
        # Format result for return
        result_json = json.dumps({
            "results": results,
            "metrics": metrics,
            "totalProcessed": processed_count,
            "processingTime": time.time() - start_time
        })
        
        # Print in a format that can be easily parsed by Node.js
        print(f"RESULT:{result_json}::END_RESULT")
        
        return {
            "results": results,
            "metrics": metrics,
            "totalProcessed": processed_count,
            "processingTime": time.time() - start_time
        }
    
    def analyze_sentiment(self, text: str, language: str = 'en') -> Dict[str, Any]:
        """
        Analyze sentiment of text using the hybrid model or fallback methods
        
        Args:
            text: Text to analyze
            language: Language code (en or tl)
            
        Returns:
            Dictionary with sentiment analysis results
        """
        # Clean and preprocess text
        cleaned_text = self.preprocess_text(text)
        
        # Use the hybrid neural network if available
        if self.torch_available and self.transformers_available:
            # This would call the actual neural network if it were available
            # Since we don't have transformers, we'll use fallbacks
            pass
        
        # Use ML-based fallback
        # Extract features and make prediction
        text_features = self.vectorizer.transform([cleaned_text])
        sentiment_probs = self.classifier.predict_proba(text_features)[0]
        sentiment_index = sentiment_probs.argmax()
        
        # Map to sentiment class and get confidence
        sentiment_classes = self.classifier.classes_
        predicted_sentiment = sentiment_classes[sentiment_index]
        confidence = sentiment_probs[sentiment_index]
        
        # Ensure higher-quality results with additional pattern matching
        # This gives more weight to specific disaster-related language patterns
        pattern_sentiment, pattern_confidence = self.pattern_match_sentiment(cleaned_text)
        
        # Use pattern match if confidence is higher
        if pattern_confidence > confidence:
            predicted_sentiment = pattern_sentiment
            confidence = pattern_confidence
            explanation = f"Pattern matching found strong indicators of {predicted_sentiment}"
        else:
            explanation = f"ML classification determined {predicted_sentiment} with {confidence:.2f} confidence"
        
        # Extract key phrases for explanation
        key_phrases = self.extract_key_phrases(cleaned_text, predicted_sentiment)
        if key_phrases:
            explanation += f"\nKey phrases: {', '.join(key_phrases)}"
        
        return {
            "sentiment": predicted_sentiment,
            "confidence": float(confidence),
            "explanation": explanation
        }
    
    def pattern_match_sentiment(self, text: str) -> Tuple[str, float]:
        """
        Use pattern matching as a fallback mechanism
        
        Args:
            text: Text to analyze
            
        Returns:
            Tuple of (sentiment, confidence)
        """
        # Convert to lowercase for matching
        text_lower = text.lower()
        
        # Count keyword matches for each sentiment
        match_counts = {sentiment: 0 for sentiment in SENTIMENT_CATEGORIES}
        matched_keywords = {sentiment: [] for sentiment in SENTIMENT_CATEGORIES}
        
        # Check for keywords
        for sentiment, keywords in SENTIMENT_KEYWORDS.items():
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    match_counts[sentiment] += 1
                    matched_keywords[sentiment].append(keyword)
        
        # Get sentiment with most matches
        if sum(match_counts.values()) > 0:
            max_count = max(match_counts.values())
            if max_count > 0:
                # Get all sentiments with max matches
                candidates = [s for s, c in match_counts.items() if c == max_count]
                # Choose the first one (could be random or use other tie-breakers)
                predicted_sentiment = candidates[0]
                # Scale confidence based on number of matches (max 0.95)
                confidence = min(0.5 + (max_count * 0.15), 0.95)
                return predicted_sentiment, confidence
        
        # Default to Neutral with low confidence if no matches
        return "Neutral", 0.3
    
    def preprocess_text(self, text: str) -> str:
        """
        Preprocess text for sentiment analysis
        
        Args:
            text: Raw text
            
        Returns:
            Preprocessed text
        """
        # Convert to string
        text = str(text)
        
        # Remove URLs
        text = re.sub(r'https?://\S+|www\.\S+', ' ', text)
        
        # Remove excess whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def extract_key_phrases(self, text: str, sentiment: str) -> List[str]:
        """
        Extract key phrases that match the predicted sentiment
        
        Args:
            text: Preprocessed text
            sentiment: Predicted sentiment
            
        Returns:
            List of key phrases
        """
        text_lower = text.lower()
        key_phrases = []
        
        # Find sentiment keywords in text
        if sentiment in SENTIMENT_KEYWORDS:
            for keyword in SENTIMENT_KEYWORDS[sentiment]:
                if keyword in text_lower:
                    # Extract context around the keyword
                    pattern = r'.{0,20}' + re.escape(keyword) + r'.{0,20}'
                    matches = re.findall(pattern, text_lower)
                    if matches:
                        key_phrases.extend(matches[:2])  # Limit to 2 matches per keyword
        
        # Limit total phrases
        return key_phrases[:3]
    
    def calculate_metrics(self, true_labels: List[str], predicted_labels: List[str], confidences: List[float]) -> Dict[str, Any]:
        """
        Calculate performance metrics for sentiment analysis
        
        Args:
            true_labels: List of true sentiment labels
            predicted_labels: List of predicted sentiment labels
            confidences: List of confidence scores
            
        Returns:
            Dictionary with metrics
        """
        from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
        
        # Calculate accuracy
        accuracy = accuracy_score(true_labels, predicted_labels)
        
        # Calculate precision, recall, and F1-score
        precision, recall, f1, support = precision_recall_fscore_support(
            true_labels, predicted_labels, average=None, labels=SENTIMENT_CATEGORIES
        )
        
        # Calculate weighted averages
        weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(
            true_labels, predicted_labels, average='weighted'
        )
        
        # Calculate confusion matrix
        conf_matrix = confusion_matrix(true_labels, predicted_labels, labels=SENTIMENT_CATEGORIES).tolist()
        
        # Calculate confidence metrics
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0
        
        # Format metrics
        metrics = {
            "accuracy": float(accuracy),
            "precision": {
                "per_class": {cat: float(p) for cat, p in zip(SENTIMENT_CATEGORIES, precision)},
                "weighted": float(weighted_precision)
            },
            "recall": {
                "per_class": {cat: float(r) for cat, r in zip(SENTIMENT_CATEGORIES, recall)},
                "weighted": float(weighted_recall)
            },
            "f1_score": {
                "per_class": {cat: float(f) for cat, f in zip(SENTIMENT_CATEGORIES, f1)},
                "weighted": float(weighted_f1)
            },
            "support": {cat: int(s) for cat, s in zip(SENTIMENT_CATEGORIES, support)},
            "confusion_matrix": {
                "matrix": conf_matrix,
                "labels": SENTIMENT_CATEGORIES
            },
            "confidence": {
                "average": float(avg_confidence),
                "distribution": {
                    "0.0-0.2": len([c for c in confidences if c <= 0.2]),
                    "0.2-0.4": len([c for c in confidences if 0.2 < c <= 0.4]),
                    "0.4-0.6": len([c for c in confidences if 0.4 < c <= 0.6]),
                    "0.6-0.8": len([c for c in confidences if 0.6 < c <= 0.8]),
                    "0.8-1.0": len([c for c in confidences if c > 0.8])
                }
            }
        }
        
        return metrics


class LocationExtractor:
    """
    Extract location information from text with a focus on Philippine locations
    """
    
    def __init__(self):
        self.location_keywords = set(PH_LOCATIONS)
        
        # Add lowercase versions for case-insensitive matching
        for location in list(self.location_keywords):
            self.location_keywords.add(location.lower())
    
    def extract(self, text: str) -> str:
        """
        Extract location from text, focusing on Philippine locations
        
        Args:
            text: Input text to analyze
            
        Returns:
            Extracted location or empty string if none found
        """
        # Simple pattern matching for known locations
        for location in PH_LOCATIONS:
            # Check for exact matches (case insensitive)
            if re.search(r'\b' + re.escape(location) + r'\b', text, re.IGNORECASE):
                return location
        
        # No location found
        return ""


class DisasterTypeExtractor:
    """
    Extract disaster type from text
    """
    
    def __init__(self):
        self.disaster_types = list(DISASTER_KEYWORDS.keys())
        self.disaster_keywords = DISASTER_KEYWORDS
    
    def extract(self, text: str) -> str:
        """
        Extract disaster type from text
        
        Args:
            text: Input text to analyze
            
        Returns:
            Extracted disaster type or "Not Specified" if none found
        """
        text_lower = text.lower()
        
        # Check each disaster type's keywords
        for disaster_type, keywords in self.disaster_keywords.items():
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    return disaster_type
        
        # No disaster type found
        return "Not Specified"


def main():
    """Main entry point for the script"""
    parser = argparse.ArgumentParser(description='Process CSV files with the PanicSense Hybrid Neural Network')
    parser.add_argument('--input', required=True, help='Path to input CSV file')
    parser.add_argument('--output', help='Path to output JSON file (optional)')
    parser.add_argument('--text-column', default='text', help='Column containing text data (default: text)')
    parser.add_argument('--batch-size', type=int, default=32, help='Batch size for processing (default: 32)')
    parser.add_argument('--validate', action='store_true', help='Validate model performance (requires sentiment column)')
    parser.add_argument('--model-path', help='Path to custom model (optional)')
    parser.add_argument('--session-id', help='Session ID for tracking (optional)')
    
    args = parser.parse_args()
    
    # Initialize processor
    processor = CSVProcessorHybrid(model_path=args.model_path)
    
    # Store session ID
    if args.session_id:
        processor.current_session_id = args.session_id
    
    # Process the CSV file
    result = processor.process_csv(
        input_file=args.input,
        output_file=args.output,
        text_column=args.text_column,
        batch_size=args.batch_size,
        validate=args.validate
    )
    
    # Print summary
    if 'error' in result:
        print(f"Error: {result['error']}")
        sys.exit(1)
    else:
        print(f"Processed {result['totalProcessed']} records in {result['processingTime']:.2f} seconds")
        if result.get('metrics'):
            print(f"Accuracy: {result['metrics']['accuracy']:.4f}")
        sys.exit(0)


if __name__ == "__main__":
    main()